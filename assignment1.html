<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>98e0ce1ebf5e4e58a42803cf5927acc1</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="task-1" class="cell markdown">
<h1>Task 1</h1>
<h3 id="step-1">Step 1</h3>
</section>
<div class="cell code" data-execution_count="2">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the CSV file.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;CTG.csv&quot;</span>, skiprows<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the relevant numerical columns.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>selected_cols <span class="op">=</span> [<span class="st">&#39;LB&#39;</span>, <span class="st">&#39;AC&#39;</span>, <span class="st">&#39;FM&#39;</span>, <span class="st">&#39;UC&#39;</span>, <span class="st">&#39;DL&#39;</span>, <span class="st">&#39;DS&#39;</span>, <span class="st">&#39;DP&#39;</span>, <span class="st">&#39;ASTV&#39;</span>, <span class="st">&#39;MSTV&#39;</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&#39;ALTV&#39;</span>, <span class="st">&#39;MLTV&#39;</span>, <span class="st">&#39;Width&#39;</span>, <span class="st">&#39;Min&#39;</span>, <span class="st">&#39;Max&#39;</span>, <span class="st">&#39;Nmax&#39;</span>, <span class="st">&#39;Nzeros&#39;</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>                 <span class="st">&#39;Mode&#39;</span>, <span class="st">&#39;Mean&#39;</span>, <span class="st">&#39;Median&#39;</span>, <span class="st">&#39;Variance&#39;</span>, <span class="st">&#39;Tendency&#39;</span>, <span class="st">&#39;NSP&#39;</span>]</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data[selected_cols].dropna()</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle the dataset.</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>data_shuffled <span class="op">=</span> data.sample(frac<span class="op">=</span><span class="fl">1.0</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into input part X and output part Y.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># &#39;NSP&#39; stands for &#39;Normal, Suspicious, Pathological&#39;</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># and is the output column.</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> data_shuffled.drop(<span class="st">&#39;NSP&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Map the diagnosis code (0-3) to a human-readable label.</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># i.e. 0 -&gt; None, 1 -&gt; normal, 2 -&gt; suspect, 3 -&gt; pathologic</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_label(y):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="va">None</span>, <span class="st">&#39;normal&#39;</span>, <span class="st">&#39;suspect&#39;</span>, <span class="st">&#39;pathologic&#39;</span>][(<span class="bu">int</span>(y))]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> data_shuffled[<span class="st">&#39;NSP&#39;</span>].<span class="bu">apply</span>(to_label)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Partition the data into training and test sets.</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>Xtrain, Xtest, Ytrain, Ytest <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>X.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="2">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LB</th>
      <th>AC</th>
      <th>FM</th>
      <th>UC</th>
      <th>DL</th>
      <th>DS</th>
      <th>DP</th>
      <th>ASTV</th>
      <th>MSTV</th>
      <th>ALTV</th>
      <th>...</th>
      <th>Width</th>
      <th>Min</th>
      <th>Max</th>
      <th>Nmax</th>
      <th>Nzeros</th>
      <th>Mode</th>
      <th>Mean</th>
      <th>Median</th>
      <th>Variance</th>
      <th>Tendency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>658</th>
      <td>130.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>24.0</td>
      <td>1.2</td>
      <td>12.0</td>
      <td>...</td>
      <td>35.0</td>
      <td>120.0</td>
      <td>155.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>134.0</td>
      <td>133.0</td>
      <td>135.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1734</th>
      <td>134.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>59.0</td>
      <td>1.2</td>
      <td>0.0</td>
      <td>...</td>
      <td>109.0</td>
      <td>80.0</td>
      <td>189.0</td>
      <td>6.0</td>
      <td>0.0</td>
      <td>150.0</td>
      <td>146.0</td>
      <td>150.0</td>
      <td>33.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1226</th>
      <td>125.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>43.0</td>
      <td>0.7</td>
      <td>31.0</td>
      <td>...</td>
      <td>21.0</td>
      <td>120.0</td>
      <td>141.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>131.0</td>
      <td>130.0</td>
      <td>132.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1808</th>
      <td>143.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>69.0</td>
      <td>0.3</td>
      <td>6.0</td>
      <td>...</td>
      <td>27.0</td>
      <td>132.0</td>
      <td>159.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>145.0</td>
      <td>144.0</td>
      <td>146.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>825</th>
      <td>152.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>62.0</td>
      <td>0.4</td>
      <td>59.0</td>
      <td>...</td>
      <td>25.0</td>
      <td>136.0</td>
      <td>161.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>159.0</td>
      <td>156.0</td>
      <td>158.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 21 columns</p>
</div>
</div>
</div>
<section id="step-2" class="cell markdown">
<h3>Step 2</h3>
</section>
<div class="cell code" data-execution_count="3">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.dummy <span class="im">import</span> DummyClassifier</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_val_score</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> DummyClassifier(strategy<span class="op">=</span><span class="st">&#39;most_frequent&#39;</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> cross_val_score(clf, Xtrain, Ytrain)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> value <span class="kw">in</span> score:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    result <span class="op">+=</span> value</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>result <span class="op">/=</span> <span class="bu">len</span>(score)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(result)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>0.7805882352941176
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="4">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingClassifier</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Perceptron</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>all_clf <span class="op">=</span> [(<span class="st">&quot;DecisionTreeClassifier&quot;</span>, DecisionTreeClassifier()),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;RandomForestClassifier&quot;</span>, RandomForestClassifier()),</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;GradientBoostingClassifier&quot;</span>, GradientBoostingClassifier()),</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;Perceptron&quot;</span>, Perceptron()),</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;LogisticRegression&quot;</span>, LogisticRegression(max_iter <span class="op">=</span> <span class="dv">100000</span>)),</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;LinearSVC&quot;</span>, LinearSVC()),</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;MLPClassifier&quot;</span>, MLPClassifier())]</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Try out all the classifiers and check their accuracy</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> clf <span class="kw">in</span> all_clf:</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    warnings.filterwarnings(<span class="st">&quot;ignore&quot;</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    score <span class="op">=</span> cross_val_score(clf[<span class="dv">1</span>], Xtrain, Ytrain)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> value <span class="kw">in</span> score:</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        result <span class="op">+=</span> value</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    result <span class="op">/=</span> <span class="bu">len</span>(score)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>clf[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>result<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output stream stdout">
<pre><code>DecisionTreeClassifier: 0.9241176470588235
RandomForestClassifier: 0.9388235294117647
GradientBoostingClassifier: 0.9494117647058824
Perceptron: 0.825294117647059
LogisticRegression: 0.8905882352941177
LinearSVC: 0.8017647058823529
MLPClassifier: 0.8858823529411766
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="5">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> all_clf[<span class="dv">2</span>][<span class="dv">1</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>clf.fit(Xtrain, Ytrain)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Yguess <span class="op">=</span> clf.predict(Xtest)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>all_clf[<span class="dv">2</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>accuracy_score(Ytest, Yguess)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>GradientBoostingClassifier: 0.9295774647887324
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>We chose the Gradient Boosting Classifier since it resulted in the
best cross validation score. Gradient Boosting Classifier is an
algorithm that uses decision trees and iteratively adds new ones until
either a satisfactory accuracy score has been achieved or a given number
of iterations has been reached. It adds new decision trees based on
previous errors in hope of that the new addition will correct the
errors.</p>
</div>
<section id="task-2" class="cell markdown">
<h1>Task 2</h1>
</section>
<div class="cell markdown">

</div>
<div class="cell code" data-execution_count="23">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecisionTreeLeaf:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, value):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> value</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This method computes the prediction for this leaf node. This will just return a constant value.</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.value</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Utility function to draw a tree visually using graphviz.</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> draw_tree(<span class="va">self</span>, graph, node_counter, names):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        node_id <span class="op">=</span> <span class="bu">str</span>(node_counter)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        val_str <span class="op">=</span> <span class="ss">f&#39;</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>value<span class="sc">:.4g}</span><span class="ss">&#39;</span> <span class="cf">if</span> <span class="bu">isinstance</span>(<span class="va">self</span>.value, <span class="bu">float</span>) <span class="cf">else</span> <span class="bu">str</span>(<span class="va">self</span>.value)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        graph.node(node_id, val_str, style<span class="op">=</span><span class="st">&#39;filled&#39;</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> node_counter<span class="op">+</span><span class="dv">1</span>, node_id</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__eq__</span>(<span class="va">self</span>, other):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(other, DecisionTreeLeaf):</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.value <span class="op">==</span> other.value</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecisionTreeBranch:</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, feature, threshold, low_subtree, high_subtree):</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature <span class="op">=</span> feature</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.threshold <span class="op">=</span> threshold</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.low_subtree <span class="op">=</span> low_subtree</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.high_subtree <span class="op">=</span> high_subtree</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For a branch node, we compute the prediction by first considering the feature, and then </span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># calling the upper or lower subtree, depending on whether the feature is or isn&#39;t greater</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># than the threshold.</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, x):</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x[<span class="va">self</span>.feature] <span class="op">&lt;=</span> <span class="va">self</span>.threshold:</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.low_subtree.predict(x)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.high_subtree.predict(x)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Utility function to draw a tree visually using graphviz.</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> draw_tree(<span class="va">self</span>, graph, node_counter, names):</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        node_counter, low_id <span class="op">=</span> <span class="va">self</span>.low_subtree.draw_tree(graph, node_counter, names)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        node_counter, high_id <span class="op">=</span> <span class="va">self</span>.high_subtree.draw_tree(graph, node_counter, names)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        node_id <span class="op">=</span> <span class="bu">str</span>(node_counter)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        fname <span class="op">=</span> <span class="ss">f&#39;F</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>feature<span class="sc">}</span><span class="ss">&#39;</span> <span class="cf">if</span> names <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> names[<span class="va">self</span>.feature]</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        lbl <span class="op">=</span> <span class="ss">f&#39;</span><span class="sc">{</span>fname<span class="sc">}</span><span class="ss"> &gt; </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>threshold<span class="sc">:.4g}</span><span class="ss">?&#39;</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        graph.node(node_id, lbl, shape<span class="op">=</span><span class="st">&#39;box&#39;</span>, fillcolor<span class="op">=</span><span class="st">&#39;yellow&#39;</span>, style<span class="op">=</span><span class="st">&#39;filled, rounded&#39;</span>)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>        graph.edge(node_id, low_id, <span class="st">&#39;False&#39;</span>)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        graph.edge(node_id, high_id, <span class="st">&#39;True&#39;</span>)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> node_counter<span class="op">+</span><span class="dv">1</span>, node_id</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> graphviz <span class="im">import</span> Digraph</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.base <span class="im">import</span> BaseEstimator, ClassifierMixin</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> abc <span class="im">import</span> ABC, abstractmethod</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DecisionTree(ABC, BaseEstimator):</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_depth):</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_depth <span class="op">=</span> max_depth</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that</span></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># we&#39;re sure that it&#39;s represented as a NumPy matrix. Then we call the recursive tree-building method</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># called make_tree (see below).</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, Y):</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(X, pd.DataFrame):</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.names <span class="op">=</span> X.columns</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> X.to_numpy()</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(X, <span class="bu">list</span>):</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.names <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> np.array(X)</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.names <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>        Y <span class="op">=</span> np.array(Y)        </span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.root <span class="op">=</span> <span class="va">self</span>.make_tree(X, Y, <span class="va">self</span>.max_depth)</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> draw_tree(<span class="va">self</span>):</span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>        graph <span class="op">=</span> Digraph()</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.root.draw_tree(graph, <span class="dv">0</span>, <span class="va">self</span>.names)</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> graph</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># By scikit-learn convention, the method *predict* computes the classification or regression output</span></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for a set of instances.</span></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>    <span class="co"># To implement it, we call a separate method that carries out the prediction for one instance.</span></span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(X, pd.DataFrame):</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> X.to_numpy()</span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [<span class="va">self</span>.predict_one(x) <span class="cf">for</span> x <span class="kw">in</span> X]</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predicting the output for one instance.</span></span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_one(<span class="va">self</span>, x):</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.root.predict(x)        </span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This is the recursive training </span></span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> make_tree(<span class="va">self</span>, X, Y, max_depth):</span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We start by computing the default value that will be used if we&#39;ll return a leaf node.</span></span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For classifiers, this will be the most common value in Y.</span></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a>        default_value <span class="op">=</span> <span class="va">self</span>.get_default_value(Y)</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First the two base cases in the recursion: is the training set completely</span></span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># homogeneous, or have we reached the maximum depth? Then we need to return a leaf.</span></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If we have reached the maximum depth, return a leaf with the majority value.</span></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> max_depth <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DecisionTreeLeaf(default_value)</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If all the instances in the remaining training set have the same output value,</span></span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return a leaf with this value.</span></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.is_homogeneous(Y):</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DecisionTreeLeaf(default_value)</span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Select the &quot;most useful&quot; feature and split threshold. To rank the &quot;usefulness&quot; of features,</span></span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we use one of the classification or regression criteria.</span></span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For each feature, we call best_split (defined in a subclass). We then maximize over the features.</span></span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a>        n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a>        _, best_feature, best_threshold <span class="op">=</span> <span class="bu">max</span>(<span class="va">self</span>.best_split(X, Y, feature) <span class="cf">for</span> feature <span class="kw">in</span> <span class="bu">range</span>(n_features))</span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> best_feature <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> DecisionTreeLeaf(default_value)</span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split the training set into subgroups, based on whether the selected feature is greater than</span></span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the threshold or not</span></span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a>        X_low, X_high, Y_low, Y_high <span class="op">=</span> <span class="va">self</span>.split_by_feature(X, Y, best_feature, best_threshold)</span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build the subtrees using a recursive call. Each subtree is associated</span></span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a>        <span class="co"># with a value of the feature.</span></span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a>        low_subtree <span class="op">=</span> <span class="va">self</span>.make_tree(X_low, Y_low, max_depth<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a>        high_subtree <span class="op">=</span> <span class="va">self</span>.make_tree(X_high, Y_high, max_depth<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> low_subtree <span class="op">==</span> high_subtree:</span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> low_subtree</span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return a decision tree branch containing the result.</span></span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)</span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Utility method that splits the data into the &quot;upper&quot; and &quot;lower&quot; part, based on a feature</span></span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and a threshold.</span></span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> split_by_feature(<span class="va">self</span>, X, Y, feature, threshold):</span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a>        low <span class="op">=</span> X[:,feature] <span class="op">&lt;=</span> threshold</span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a>        high <span class="op">=</span> <span class="op">~</span>low</span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X[low], X[high], Y[low], Y[high]</span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The following three methods need to be implemented by the classification and regression subclasses.</span></span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_default_value(<span class="va">self</span>, Y):</span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> is_homogeneous(<span class="va">self</span>, Y):</span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> best_split(<span class="va">self</span>, X, Y, feature):</span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span>
<span id="cb8-157"><a href="#cb8-157" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb8-158"><a href="#cb8-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TreeClassifier(DecisionTree, ClassifierMixin):</span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, max_depth<span class="op">=</span><span class="dv">10</span>, criterion<span class="op">=</span><span class="st">&#39;maj_sum&#39;</span>):</span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(max_depth)</span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.criterion <span class="op">=</span> criterion</span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, Y):</span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For decision tree classifiers, there are some different ways to measure</span></span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the homogeneity of subsets.</span></span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.criterion <span class="op">==</span> <span class="st">&#39;maj_sum&#39;</span>:</span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.criterion_function <span class="op">=</span> majority_sum_scorer</span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.criterion <span class="op">==</span> <span class="st">&#39;info_gain&#39;</span>:</span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.criterion_function <span class="op">=</span> info_gain_scorer</span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.criterion <span class="op">==</span> <span class="st">&#39;gini&#39;</span>:</span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.criterion_function <span class="op">=</span> gini_scorer</span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="ss">f&#39;Unknown criterion: </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>criterion<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().fit(X, Y)</span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classes_ <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">set</span>(Y))</span>
<span id="cb8-178"><a href="#cb8-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-179"><a href="#cb8-179" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Select a default value that is going to be used if we decide to make a leaf.</span></span>
<span id="cb8-180"><a href="#cb8-180" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We will select the most common value.</span></span>
<span id="cb8-181"><a href="#cb8-181" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_default_value(<span class="va">self</span>, Y):</span>
<span id="cb8-182"><a href="#cb8-182" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.class_distribution <span class="op">=</span> Counter(Y)</span>
<span id="cb8-183"><a href="#cb8-183" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.class_distribution.most_common(<span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb8-184"><a href="#cb8-184" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-185"><a href="#cb8-185" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Checks whether a set of output values is homogeneous. In the classification case, </span></span>
<span id="cb8-186"><a href="#cb8-186" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this means that all output values are identical.</span></span>
<span id="cb8-187"><a href="#cb8-187" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We assume that we called get_default_value just before, so that we can access</span></span>
<span id="cb8-188"><a href="#cb8-188" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the class_distribution attribute. If the class distribution contains just one item,</span></span>
<span id="cb8-189"><a href="#cb8-189" aria-hidden="true" tabindex="-1"></a>    <span class="co"># this means that the set is homogeneous.</span></span>
<span id="cb8-190"><a href="#cb8-190" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> is_homogeneous(<span class="va">self</span>, Y):</span>
<span id="cb8-191"><a href="#cb8-191" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.class_distribution) <span class="op">==</span> <span class="dv">1</span></span>
<span id="cb8-192"><a href="#cb8-192" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-193"><a href="#cb8-193" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Finds the best splitting point for a given feature. We&#39;ll keep frequency tables (Counters)</span></span>
<span id="cb8-194"><a href="#cb8-194" aria-hidden="true" tabindex="-1"></a>    <span class="co"># for the upper and lower parts, and then compute the impurity criterion using these tables.</span></span>
<span id="cb8-195"><a href="#cb8-195" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In the end, we return a triple consisting of</span></span>
<span id="cb8-196"><a href="#cb8-196" aria-hidden="true" tabindex="-1"></a>    <span class="co"># - the best score we found, according to the criterion we&#39;re using</span></span>
<span id="cb8-197"><a href="#cb8-197" aria-hidden="true" tabindex="-1"></a>    <span class="co"># - the id of the feature</span></span>
<span id="cb8-198"><a href="#cb8-198" aria-hidden="true" tabindex="-1"></a>    <span class="co"># - the threshold for the best split</span></span>
<span id="cb8-199"><a href="#cb8-199" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> best_split(<span class="va">self</span>, X, Y, feature):</span>
<span id="cb8-200"><a href="#cb8-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-201"><a href="#cb8-201" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a list of input-output pairs, where we have sorted</span></span>
<span id="cb8-202"><a href="#cb8-202" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in ascending order by the input feature we&#39;re considering.</span></span>
<span id="cb8-203"><a href="#cb8-203" aria-hidden="true" tabindex="-1"></a>        sorted_indices <span class="op">=</span> np.argsort(X[:, feature])        </span>
<span id="cb8-204"><a href="#cb8-204" aria-hidden="true" tabindex="-1"></a>        X_sorted <span class="op">=</span> <span class="bu">list</span>(X[sorted_indices, feature])</span>
<span id="cb8-205"><a href="#cb8-205" aria-hidden="true" tabindex="-1"></a>        Y_sorted <span class="op">=</span> <span class="bu">list</span>(Y[sorted_indices])</span>
<span id="cb8-206"><a href="#cb8-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-207"><a href="#cb8-207" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">len</span>(Y)</span>
<span id="cb8-208"><a href="#cb8-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-209"><a href="#cb8-209" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The frequency tables corresponding to the parts *before and including*</span></span>
<span id="cb8-210"><a href="#cb8-210" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and *after* the current element.</span></span>
<span id="cb8-211"><a href="#cb8-211" aria-hidden="true" tabindex="-1"></a>        low_distr <span class="op">=</span> Counter()</span>
<span id="cb8-212"><a href="#cb8-212" aria-hidden="true" tabindex="-1"></a>        high_distr <span class="op">=</span> Counter(Y)</span>
<span id="cb8-213"><a href="#cb8-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-214"><a href="#cb8-214" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Keep track of the best result we&#39;ve seen so far.</span></span>
<span id="cb8-215"><a href="#cb8-215" aria-hidden="true" tabindex="-1"></a>        max_score <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb8-216"><a href="#cb8-216" aria-hidden="true" tabindex="-1"></a>        max_i <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-217"><a href="#cb8-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-218"><a href="#cb8-218" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Go through all the positions (excluding the last position).</span></span>
<span id="cb8-219"><a href="#cb8-219" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb8-220"><a href="#cb8-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-221"><a href="#cb8-221" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Input and output at the current position.</span></span>
<span id="cb8-222"><a href="#cb8-222" aria-hidden="true" tabindex="-1"></a>            x_i <span class="op">=</span> X_sorted[i]</span>
<span id="cb8-223"><a href="#cb8-223" aria-hidden="true" tabindex="-1"></a>            y_i <span class="op">=</span> Y_sorted[i]</span>
<span id="cb8-224"><a href="#cb8-224" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb8-225"><a href="#cb8-225" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update the frequency tables.</span></span>
<span id="cb8-226"><a href="#cb8-226" aria-hidden="true" tabindex="-1"></a>            low_distr[y_i] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-227"><a href="#cb8-227" aria-hidden="true" tabindex="-1"></a>            high_distr[y_i] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb8-228"><a href="#cb8-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-229"><a href="#cb8-229" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If the input is equal to the input at the next position, we will</span></span>
<span id="cb8-230"><a href="#cb8-230" aria-hidden="true" tabindex="-1"></a>            <span class="co"># not consider a split here.</span></span>
<span id="cb8-231"><a href="#cb8-231" aria-hidden="true" tabindex="-1"></a>            <span class="co">#x_next = XY[i+1][0]</span></span>
<span id="cb8-232"><a href="#cb8-232" aria-hidden="true" tabindex="-1"></a>            x_next <span class="op">=</span> X_sorted[i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb8-233"><a href="#cb8-233" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> x_i <span class="op">==</span> x_next:</span>
<span id="cb8-234"><a href="#cb8-234" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb8-235"><a href="#cb8-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-236"><a href="#cb8-236" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Compute the homogeneity criterion for a split at this position.</span></span>
<span id="cb8-237"><a href="#cb8-237" aria-hidden="true" tabindex="-1"></a>            score <span class="op">=</span> <span class="va">self</span>.criterion_function(i<span class="op">+</span><span class="dv">1</span>, low_distr, n<span class="op">-</span>i<span class="op">-</span><span class="dv">1</span>, high_distr)</span>
<span id="cb8-238"><a href="#cb8-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-239"><a href="#cb8-239" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If this is the best split, remember it.</span></span>
<span id="cb8-240"><a href="#cb8-240" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> score <span class="op">&gt;</span> max_score:</span>
<span id="cb8-241"><a href="#cb8-241" aria-hidden="true" tabindex="-1"></a>                max_score <span class="op">=</span> score</span>
<span id="cb8-242"><a href="#cb8-242" aria-hidden="true" tabindex="-1"></a>                max_i <span class="op">=</span> i</span>
<span id="cb8-243"><a href="#cb8-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-244"><a href="#cb8-244" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If we didn&#39;t find any split (meaning that all inputs are identical), return</span></span>
<span id="cb8-245"><a href="#cb8-245" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a dummy value.</span></span>
<span id="cb8-246"><a href="#cb8-246" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> max_i <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-247"><a href="#cb8-247" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="op">-</span>np.inf, <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb8-248"><a href="#cb8-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-249"><a href="#cb8-249" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Otherwise, return the best split we found and its score.</span></span>
<span id="cb8-250"><a href="#cb8-250" aria-hidden="true" tabindex="-1"></a>        split_point <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>(X_sorted[max_i] <span class="op">+</span> X_sorted[max_i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb8-251"><a href="#cb8-251" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> max_score, feature, split_point</span>
<span id="cb8-252"><a href="#cb8-252" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> majority_sum_scorer(n_low, low_distr, n_high, high_distr):</span>
<span id="cb8-253"><a href="#cb8-253" aria-hidden="true" tabindex="-1"></a>    maj_sum_low <span class="op">=</span> low_distr.most_common(<span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb8-254"><a href="#cb8-254" aria-hidden="true" tabindex="-1"></a>    maj_sum_high <span class="op">=</span> high_distr.most_common(<span class="dv">1</span>)[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb8-255"><a href="#cb8-255" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> maj_sum_low <span class="op">+</span> maj_sum_high</span>
<span id="cb8-256"><a href="#cb8-256" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-257"><a href="#cb8-257" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> entropy(distr):</span>
<span id="cb8-258"><a href="#cb8-258" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">sum</span>(distr.values())</span>
<span id="cb8-259"><a href="#cb8-259" aria-hidden="true" tabindex="-1"></a>    ps <span class="op">=</span> [n_i<span class="op">/</span>n <span class="cf">for</span> n_i <span class="kw">in</span> distr.values()]</span>
<span id="cb8-260"><a href="#cb8-260" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="bu">sum</span>(p<span class="op">*</span>np.log2(p) <span class="cf">if</span> p <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> p <span class="kw">in</span> ps)</span>
<span id="cb8-261"><a href="#cb8-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-262"><a href="#cb8-262" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> info_gain_scorer(n_low, low_distr, n_high, high_distr):</span>
<span id="cb8-263"><a href="#cb8-263" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(n_low<span class="op">*</span>entropy(low_distr)<span class="op">+</span>n_high<span class="op">*</span>entropy(high_distr))<span class="op">/</span>(n_low<span class="op">+</span>n_high)</span>
<span id="cb8-264"><a href="#cb8-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-265"><a href="#cb8-265" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gini_impurity(distr):</span>
<span id="cb8-266"><a href="#cb8-266" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">sum</span>(distr.values())</span>
<span id="cb8-267"><a href="#cb8-267" aria-hidden="true" tabindex="-1"></a>    ps <span class="op">=</span> [n_i<span class="op">/</span>n <span class="cf">for</span> n_i <span class="kw">in</span> distr.values()]</span>
<span id="cb8-268"><a href="#cb8-268" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span><span class="bu">sum</span>(p<span class="op">**</span><span class="dv">2</span> <span class="cf">for</span> p <span class="kw">in</span> ps)</span>
<span id="cb8-269"><a href="#cb8-269" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-270"><a href="#cb8-270" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gini_scorer(n_low, low_distr, n_high, high_distr):</span>
<span id="cb8-271"><a href="#cb8-271" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(n_low<span class="op">*</span>gini_impurity(low_distr)<span class="op">+</span>n_high<span class="op">*</span>gini_impurity(high_distr))<span class="op">/</span>(n_low<span class="op">+</span>n_high)</span>
<span id="cb8-272"><a href="#cb8-272" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-273"><a href="#cb8-273" aria-hidden="true" tabindex="-1"></a>max_score <span class="op">=</span> {<span class="st">&#39;Max_depth&#39;</span>: <span class="dv">0</span>,</span>
<span id="cb8-274"><a href="#cb8-274" aria-hidden="true" tabindex="-1"></a>             <span class="st">&#39;Accuracy&#39;</span>: <span class="dv">0</span>}</span>
<span id="cb8-275"><a href="#cb8-275" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">10</span>):</span>
<span id="cb8-276"><a href="#cb8-276" aria-hidden="true" tabindex="-1"></a>    cls <span class="op">=</span> TreeClassifier(max_depth<span class="op">=</span>i, criterion<span class="op">=</span><span class="st">&#39;gini&#39;</span>)</span>
<span id="cb8-277"><a href="#cb8-277" aria-hidden="true" tabindex="-1"></a>    cls.fit(Xtrain, Ytrain)</span>
<span id="cb8-278"><a href="#cb8-278" aria-hidden="true" tabindex="-1"></a>    Yguess <span class="op">=</span> clf.predict(Xtest)</span>
<span id="cb8-279"><a href="#cb8-279" aria-hidden="true" tabindex="-1"></a>    current_score <span class="op">=</span> accuracy_score(Ytest, Yguess)</span>
<span id="cb8-280"><a href="#cb8-280" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (current_score <span class="op">&gt;</span> max_score[<span class="st">&#39;Accuracy&#39;</span>]):</span>
<span id="cb8-281"><a href="#cb8-281" aria-hidden="true" tabindex="-1"></a>        max_score[<span class="st">&#39;Max_depth&#39;</span>] <span class="op">=</span> i</span>
<span id="cb8-282"><a href="#cb8-282" aria-hidden="true" tabindex="-1"></a>        max_score[<span class="st">&#39;Accuracy&#39;</span>] <span class="op">=</span> current_score</span>
<span id="cb8-283"><a href="#cb8-283" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(max_score)</span>
<span id="cb8-284"><a href="#cb8-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-285"><a href="#cb8-285" aria-hidden="true" tabindex="-1"></a>cls_draw <span class="op">=</span> TreeClassifier(max_depth<span class="op">=</span><span class="dv">3</span>, criterion<span class="op">=</span><span class="st">&#39;gini&#39;</span>)</span>
<span id="cb8-286"><a href="#cb8-286" aria-hidden="true" tabindex="-1"></a>cls_draw.fit(Xtrain, Ytrain)</span>
<span id="cb8-287"><a href="#cb8-287" aria-hidden="true" tabindex="-1"></a>cls_draw.draw_tree()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>{&#39;Max_depth&#39;: 0, &#39;Accuracy&#39;: 0.9295774647887324}
</code></pre>
</div>
<div class="output execute_result" data-execution_count="23">
<p><img
src="vertopal_22a639f7e1dc47119545ff2ef12ed838/95af22250d72a3cd9953f80e6d659e3bd89ecab6.svg" /></p>
</div>
</div>
<div class="cell markdown">
<p>We got the same accuracy score for depths from 0 up to 10 which was
'Accuracy': 0.9295774647887324. The same score was achieved for
max_depth = 100,1000.</p>
</div>
<section id="task-3" class="cell markdown">
<h1>Task 3</h1>
</section>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPRegressor</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.dummy <span class="im">import</span> DummyRegressor</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_validate</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the CSV file using Pandas.</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>alldata <span class="op">=</span> pd.read_csv(<span class="st">&#39;sberbank.csv&#39;</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the timestamp string to an integer representing the year.</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_year(timestamp):</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">int</span>(timestamp[:<span class="dv">4</span>])</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>alldata[<span class="st">&#39;year&#39;</span>] <span class="op">=</span> alldata.timestamp.<span class="bu">apply</span>(get_year)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Select the 9 input columns and the output column.</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>selected_columns <span class="op">=</span> [<span class="st">&#39;price_doc&#39;</span>, <span class="st">&#39;year&#39;</span>, <span class="st">&#39;full_sq&#39;</span>, <span class="st">&#39;life_sq&#39;</span>, <span class="st">&#39;floor&#39;</span>, <span class="st">&#39;num_room&#39;</span>, <span class="st">&#39;kitch_sq&#39;</span>, <span class="st">&#39;full_all&#39;</span>]</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>alldata <span class="op">=</span> alldata[selected_columns]</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>alldata <span class="op">=</span> alldata.dropna()</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Shuffle.</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>alldata_shuffled <span class="op">=</span> alldata.sample(frac<span class="op">=</span><span class="fl">1.0</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Separate the input and output columns.</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>X_2 <span class="op">=</span> alldata_shuffled.drop(<span class="st">&#39;price_doc&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="co"># For the output, we&#39;ll use the log of the sales price.</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>Y_2 <span class="op">=</span> alldata_shuffled[<span class="st">&#39;price_doc&#39;</span>].<span class="bu">apply</span>(np.log)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test sets.</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>Xtrain_2, Xtest_2, Ytrain_2, Ytest_2 <span class="op">=</span> train_test_split(X_2, Y_2, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>m1 <span class="op">=</span> DummyRegressor()</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>cross_validate(m1, Xtrain_2, Ytrain_2, scoring<span class="op">=</span><span class="st">&#39;neg_mean_squared_error&#39;</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>all_regr <span class="op">=</span> [(<span class="st">&quot;LinearRegression&quot;</span>, LinearRegression()),</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;Ridge&quot;</span>, Ridge()),</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;Lasso&quot;</span>, Lasso()),</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;DecisionTreeRegressor&quot;</span>, DecisionTreeRegressor()),</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;RandomForestRegressor&quot;</span>, RandomForestRegressor()),</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;GradientBoostingRegressor&quot;</span>, GradientBoostingRegressor()),</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>           (<span class="st">&quot;MLPRegressor&quot;</span>, MLPRegressor())]</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> regr <span class="kw">in</span> all_regr:</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    regr[<span class="dv">1</span>].fit(Xtrain_2, Ytrain_2)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>regr[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>mean_squared_error(Ytest_2, regr[<span class="dv">1</span>].predict(Xtest_2))<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output stream stdout">
<pre><code>LinearRegression: 0.31558903970037583
Ridge: 0.31559023545809295
Lasso: 0.32434045737369277
DecisionTreeRegressor: 0.5757639540921918
RandomForestRegressor: 0.29883104936385896
GradientBoostingRegressor: 0.2713611346226758
MLPRegressor: 888328.9479717036
</code></pre>
</div>
</div>
</body>
</html>
