{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA4: Implementing linear classifiers\n",
    "\n",
    "Participants: Arvid Nyberg and Alfred Karlsson"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise question\n",
    "\n",
    "The second training set can be visualized as:\n",
    "\n",
    "              July   December  \n",
    "       Sydney rain   sun  \n",
    "       Paris  sun    rain\n",
    "\n",
    "There is no way to draw a straight line that separates the two categories, which means they are linearly inseparable.\n",
    "Since both the Perceptron and LinearSVC are linear classifiers they fail to \"memorize\" the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the SVC\n",
    "\n",
    "We made a copy of the Perceptron algorithm and modified it to implement the Pegasos algorithm. The modifications made to the perception class is changing the algorithm according to the pseudocode given in a4_clarification.pdf. Furthermore the variable lambda was added and given the value 1/N where N is the number of instances of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pegasos(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the Pegasos learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the Pegasos learning algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        #Set lambda to 1/N where N is number of instances in the training set\n",
    "        n_instances = X.shape[0]\n",
    "        lmbda = 1/n_instances\n",
    "\n",
    "        # Pegasos algorithm:\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "\n",
    "            # Select a random training instance.\n",
    "            index = random.randint(0, len(X)-1)\n",
    "            x, y = X[index], Ye[index]\n",
    "            #Setting the step length\n",
    "            t += 1\n",
    "            n = 1/(lmbda*t)\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # Stochastic gradient descent with hinge loss\n",
    "            if y*score < 1:\n",
    "                self.w = (1-n*lmbda)*self.w + n*y*x\n",
    "            else:\n",
    "                self.w = (1-n*lmbda)*self.w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "The difference between the Pegasos algorithm and this implementation of LR is just the loss funcion.\n",
    "The gradient of the log loss is the same as the gradient of the hinge loss woth an added denomiator.\n",
    "The log loss is also continuous and non-zero for score = 1 only one case is needed.  \n",
    "  \n",
    "To implement the LR we have added the demoninator of the gradient of the log loss and removed the if-clause of the stochastic gradient descent.\n",
    "Otherwise, the implementation is the same as Pegasos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the LogisticRegression algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the LogisticRegression algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        #Set lambda to 1/N where N is number of instances in the training set\n",
    "        n_instances = X.shape[0]\n",
    "        lmbda = 1/n_instances\n",
    "\n",
    "        # Logistic Regression algorithm:\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "\n",
    "            # Select a random training instance.\n",
    "            index = random.randint(0, len(X)-1)\n",
    "            x, y = X[index], Ye[index]\n",
    "            #Setting the step length\n",
    "            t += 1\n",
    "            n = 1/(lmbda*t)\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # Stochastic gradient descent with log loss\n",
    "            self.w = (1-n*lmbda)*self.w + n*x*y/(1+np.exp(y*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('pa4/data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iter = 100000\n",
      "Training time: 4.31 sec.\n",
      "Accuracy: 0.8359.\n",
      "Training time: 5.48 sec.\n",
      "Accuracy: 0.8326.\n",
      "\n",
      "n_iter = 200000\n",
      "Training time: 5.35 sec.\n",
      "Accuracy: 0.8326.\n",
      "Training time: 8.49 sec.\n",
      "Accuracy: 0.8347.\n",
      "\n",
      "n_iter = 500000\n",
      "Training time: 10.78 sec.\n",
      "Accuracy: 0.8347.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arvid\\AppData\\Local\\Temp\\ipykernel_24680\\2947770861.py:52: RuntimeWarning: overflow encountered in exp\n",
      "  self.w = (1-n*lmbda)*self.w + n*x*y/(1+np.exp(y*score))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 18.24 sec.\n",
      "Accuracy: 0.8376.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test the Pegasos algorithm and Logistic Regression for different values of n_iter\n",
    "for n_iter in [100000, 200000, 500000]:\n",
    "    print('n_iter =', n_iter)\n",
    "    for classifier in [Pegasos(n_iter=n_iter), LogisticRegression(n_iter=n_iter)]:\n",
    "        pipeline = make_pipeline(\n",
    "            TfidfVectorizer(),\n",
    "            SelectKBest(k=1000),\n",
    "            Normalizer(),\n",
    "            classifier\n",
    "        )\n",
    "        t0 = time.time()\n",
    "        pipeline.fit(Xtrain, Ytrain)\n",
    "        t1 = time.time()\n",
    "        print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "        Yguess = pipeline.predict(Xtest)\n",
    "        print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these values of n_iter the model perform about the same, while the Logistic Regression takes longer time because of its computational complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
